{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pos\n",
    "\n",
    "Here we extract useful pos tags from the title/meta description."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting it up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../modules/')\n",
    "from webpage import *\n",
    "import nlp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Looking at meta data\n",
    "Let's see if we can find something interesting in there"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\anaconda\\lib\\site-packages\\bs4\\__init__.py:166: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "To get rid of this warning, change this:\n",
      "\n",
      " BeautifulSoup([your markup])\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup([your markup], \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n",
      "../modules/webpage.py:93: UserWarning: Unable to correctly parse given URL:http://www.nytimes.com/2015/09/11/world/netanyahu-makes-quick-pivot-from-loss-on-iran-deal.html\n",
      "  warn('Unable to correctly parse given URL:%s' %self.url)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None, None, None, None, None, None]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "webpages ={ website:Webpage(test_url[website]) for website in test_url}\n",
    "[page.load() for page in webpages.values()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__doc__',\n",
       " '__goose_article__',\n",
       " '__init__',\n",
       " '__module__',\n",
       " '_valid_text_',\n",
       " 'html',\n",
       " 'load',\n",
       " 'meta_description',\n",
       " 'meta_keywords',\n",
       " 'set_url',\n",
       " 'tags',\n",
       " 'text',\n",
       " 'title',\n",
       " 'url']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(webpages['amazon'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Georgia Institute of Technology College of Computing - Wikipedia, the free encyclopedia',\n",
       "  '',\n",
       "  ''),\n",
       " ('F.C. Barcelona News and Discussion  /r/Barca',\n",
       "  'reddit: the front page of the internet',\n",
       "  'reddit, reddit.com, vote, comment, submit'),\n",
       " (\"I'm scared of Louis van Gaal's 'bulldog face', admits Manchester United's Marcos Rojo\",\n",
       "  'Manchester United defender Marcos Rojo admits hes scared of Louis van Gaals bulldog face.',\n",
       "  ''),\n",
       " ('Barca look to continue success against Atleti',\n",
       "  \"<p>Atletico Madrid and Barcelona will meet in an early-season titan clash, one that ESPN FC's Gab Marcotti feels is not a make or break matchup.</p>\",\n",
       "  u'Latest,Atletico Madrid,Spanish Primera Divisi\\xf3n,Barcelona,XBOX 360'),\n",
       " ('', '', u''),\n",
       " ('How to Introduce Your Indoorsy Friend to the Outdoors',\n",
       "  \"Share your passion for the outdoors and introduce a friend to hike and camp--don't forget to start slow.\",\n",
       "  ''),\n",
       " ('Amazon.com: Cuisinart CPT-122 Compact 2-Slice Toaster: Kitchen & Dining',\n",
       "  'Online Shopping for Kitchen Small Appliances from a great selection of Coffee Machines, Blenders, Juicers, Ovens, Specialty Appliances, & more at everyday low prices',\n",
       "  'toaster'),\n",
       " ('Man behind NSA leaks says he did it to safeguard privacy, liberty - Politics.com',\n",
       "  'Edward Snowden might never live in the U.S. as a free man again after leaking secrets about a U.S. surveillance program',\n",
       "  'politics, Man behind NSA leaks says he did it to safeguard privacy, liberty - CNNPolitics.com')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(page.title,page.meta_description,page.meta_keywords) for page in webpages.values()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  Amazon.com/NN\n",
      "  :/:\n",
      "  Cuisinart/NNP\n",
      "  CPT-122/-NONE-\n",
      "  Compact/NNP\n",
      "  2-Slice/CD\n",
      "  Toaster/NNP\n",
      "  :/:\n",
      "  Kitchen/NNP\n",
      "  &/CC\n",
      "  Dining/NNP) ['__add__', '__class__', '__contains__', '__delattr__', '__delitem__', '__delslice__', '__dict__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__getslice__', '__gt__', '__hash__', '__iadd__', '__imul__', '__init__', '__iter__', '__le__', '__len__', '__lt__', '__module__', '__mul__', '__ne__', '__new__', '__radd__', '__reduce__', '__reduce_ex__', '__repr__', '__reversed__', '__rmul__', '__setattr__', '__setitem__', '__setslice__', '__sizeof__', '__str__', '__subclasshook__', '__unicode__', '__weakref__', '_frozen_class', '_get_node', '_label', '_parse_error', '_pformat_flat', '_repr_png_', '_set_node', 'append', 'chomsky_normal_form', 'collapse_unary', 'convert', 'copy', 'count', 'draw', 'extend', 'flatten', 'freeze', 'fromstring', 'height', 'index', 'insert', 'label', 'leaf_treeposition', 'leaves', 'node', 'pformat', 'pformat_latex_qtree', 'pop', 'pos', 'pprint', 'pretty_print', 'productions', 'remove', 'reverse', 'set_label', 'sort', 'subtrees', 'treeposition_spanning_leaves', 'treepositions', 'un_chomsky_normal_form', 'unicode_repr']\n"
     ]
    }
   ],
   "source": [
    "for tree in nlp.sent_chunker('Amazon.com: Cuisinart CPT-122 Compact 2-Slice Toaster: Kitchen & Dining'):\n",
    "    print tree,dir(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reduce POS Tags\n",
    "\n",
    "if There is a chain of nouns [ proper 'NNP'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "webpage_pos_tags = [nlp.pos_tag(page.title) for page in webpages.values()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def merge_pos_temp(temp_array):\n",
    "    \"\"\" Merge tokens from the temp array into one token_tag tuple \"\"\"\n",
    "    \n",
    "    merged_token = ' '.join(token_tag[0] for token_tag in temp_array)\n",
    "\n",
    "\n",
    "    return merged_token,'NNP'\n",
    "\n",
    "def merge_pos_tags(text_ner_tags):\n",
    "    \"\"\" Combine adjacent tokens with same NER tag into one token\"\"\"\n",
    "    \n",
    "    merged_ner_tags = []\n",
    "    temp_array = []\n",
    "    \n",
    "    for token_tag in text_ner_tags:\n",
    "        tag = token_tag[1]\n",
    "        \n",
    "        \n",
    "        if tag == 'NNP':\n",
    "            # This gonna go to the temp\n",
    "            temp_array.append(token_tag)\n",
    "            \n",
    "        elif tag in {'IN','-NONE-','CD'}: # continue\n",
    "            # See if there something in temp, \n",
    "            if len(temp_array) != 0:\n",
    "                #    if yes add it to it\n",
    "                temp_array.append(token_tag)\n",
    "            else:\n",
    "                #    if no add this to result\n",
    "                merged_ner_tags.append(token_tag)\n",
    "        else: \n",
    "            # Empty the temp\n",
    "            if len(temp_array) != 0:\n",
    "                merged_token,merged_tag = merge_pos_temp(temp_array)\n",
    "                merged_ner_tags.append((merged_token,merged_tag))\n",
    "                temp_array = []\n",
    "            \n",
    "            # add this to the result\n",
    "            merged_ner_tags.append(token_tag)\n",
    "        \n",
    "        #print token_tag,tag,temp_array,merged_ner_tags\n",
    "        \n",
    "    if len(temp_array) != 0:\n",
    "        merged_token,merged_tag = merge_pos_temp(temp_array)\n",
    "        merged_ner_tags.append((merged_token,merged_tag))\n",
    "    return merged_ner_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Georgia Institute of Technology College of Computing', 'NNP'), ('-', ':'), ('Wikipedia', 'NNP'), (',', ','), ('the', 'DT'), ('free', 'JJ'), ('encyclopedia', 'NN')]\n",
      "[('F.C', 'JJ'), ('.', '.'), ('Barcelona News', 'NNP'), ('and', 'CC'), ('Discussion /r/Barca', 'NNP')]\n",
      "[('I', 'PRP'), (\"'m\", 'VBP'), ('scared', 'JJ'), ('of', 'IN'), ('Louis van Gaal', 'NNP'), (\"'s\", 'POS'), (\"'bulldog\", 'JJ'), ('face', 'NN'), (\"'\", \"''\"), (',', ','), ('admits', 'VBZ'), ('Manchester United', 'NNP'), (\"'s\", 'POS'), ('Marcos Rojo', 'NNP')]\n",
      "[('Barca', 'NNP'), ('look', 'VBD'), ('to', 'TO'), ('continue', 'VB'), ('success', 'NN'), ('against', 'IN'), ('Atleti', 'NNP')]\n",
      "[]\n",
      "[('How', 'WRB'), ('to', 'TO'), ('Introduce Your Indoorsy Friend', 'NNP'), ('to', 'TO'), ('the', 'DT'), ('Outdoors', 'NNP')]\n",
      "[('Amazon.com', 'NN'), (':', ':'), ('Cuisinart CPT-122 Compact 2-Slice Toaster', 'NNP'), (':', ':'), ('Kitchen', 'NNP'), ('&', 'CC'), ('Dining', 'NNP')]\n",
      "[('Man', 'NN'), ('behind', 'IN'), ('NSA', 'NNP'), ('leaks', 'NNS'), ('says', 'VBZ'), ('he', 'PRP'), ('did', 'VBD'), ('it', 'PRP'), ('to', 'TO'), ('safeguard', 'VB'), ('privacy', 'NN'), (',', ','), ('liberty', 'NN'), ('-', ':'), ('Politics.com', 'JJ')]\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(webpage_pos_tags)):\n",
    "    print merge_pos_tags(webpage_pos_tags[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining it\n",
    "```\n",
    "Input :  text\n",
    "Output: List of token-tag tuples\n",
    "\n",
    "Returns pos tags with combined Proper Nouns\n",
    "\n",
    "\n",
    "Idea : The proper nouns present in the title are a very strong indicator of what is there in the text\n",
    "But so are the other nouns,... Running out of time for adding support for all nouns\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reduced_pos_tag(text):\n",
    "    \"\"\" Generate POS tags and combine proper nouns\n",
    "    \n",
    "    Input :  text\n",
    "    Output: List of token-tag tuples\n",
    "\n",
    "    Returns pos tags with combined Proper Nouns\n",
    "\n",
    "\n",
    "    Idea : The proper nouns present in the title are a very strong indicator of what is there in the text\n",
    "    But so are the other nouns,... Running out of time for adding support for all nouns\n",
    "    \n",
    "    \"\"\"\n",
    "    text_pos_tags = nlp.pos_tag(text)\n",
    "    return merge_pos_tags(text_pos_tags)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Georgia Institute of Technology College of Computing', 'NNP'), ('-', ':'), ('Wikipedia', 'NNP'), (',', ','), ('the', 'DT'), ('free', 'JJ'), ('encyclopedia', 'NN')]\n",
      "[('F.C', 'JJ'), ('.', '.'), ('Barcelona News', 'NNP'), ('and', 'CC'), ('Discussion /r/Barca', 'NNP')]\n",
      "[('I', 'PRP'), (\"'m\", 'VBP'), ('scared', 'JJ'), ('of', 'IN'), ('Louis van Gaal', 'NNP'), (\"'s\", 'POS'), (\"'bulldog\", 'JJ'), ('face', 'NN'), (\"'\", \"''\"), (',', ','), ('admits', 'VBZ'), ('Manchester United', 'NNP'), (\"'s\", 'POS'), ('Marcos Rojo', 'NNP')]\n",
      "[('Barca', 'NNP'), ('look', 'VBD'), ('to', 'TO'), ('continue', 'VB'), ('success', 'NN'), ('against', 'IN'), ('Atleti', 'NNP')]\n",
      "[]\n",
      "[('How', 'WRB'), ('to', 'TO'), ('Introduce Your Indoorsy Friend', 'NNP'), ('to', 'TO'), ('the', 'DT'), ('Outdoors', 'NNP')]\n",
      "[('Amazon.com', 'NN'), (':', ':'), ('Cuisinart CPT-122 Compact 2-Slice Toaster', 'NNP'), (':', ':'), ('Kitchen', 'NNP'), ('&', 'CC'), ('Dining', 'NNP')]\n",
      "[('Man', 'NN'), ('behind', 'IN'), ('NSA', 'NNP'), ('leaks', 'NNS'), ('says', 'VBZ'), ('he', 'PRP'), ('did', 'VBD'), ('it', 'PRP'), ('to', 'TO'), ('safeguard', 'VB'), ('privacy', 'NN'), (',', ','), ('liberty', 'NN'), ('-', ':'), ('Politics.com', 'JJ')]\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(webpages)):\n",
    "    print reduced_pos_tag(webpages.values()[i].title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[('reddit', 'NN'), (':', ':'), ('the', 'DT'), ('front', 'NN'), ('page', 'NN'), ('of', 'IN'), ('the', 'DT'), ('internet', 'NN')]\n",
      "[('Manchester United', 'NNP'), ('defender', 'NN'), ('Marcos Rojo', 'NNP'), ('admits', 'VBZ'), ('hes', 'NNS'), ('scared', 'VBN'), ('of', 'IN'), ('Louis van Gaals', 'NNP'), ('bulldog', 'NN'), ('face', 'NN'), ('.', '.')]\n",
      "[('<', 'NN'), ('p', 'NN'), ('>', ':'), ('Atletico Madrid', 'NNP'), ('and', 'CC'), ('Barcelona', 'NNP'), ('will', 'MD'), ('meet', 'VB'), ('in', 'IN'), ('an', 'DT'), ('early-season', 'NN'), ('titan', 'NN'), ('clash', 'NN'), (',', ','), ('one', 'CD'), ('that', 'WDT'), ('ESPN FC', 'NNP'), (\"'s\", 'POS'), ('Gab Marcotti', 'NNP'), ('feels', 'VBZ'), ('is', 'VBZ'), ('not', 'RB'), ('a', 'DT'), ('make', 'NN'), ('or', 'CC'), ('break', 'VB'), ('matchup. < /p >', 'NNP')]\n",
      "[]\n",
      "[('Share', 'NN'), ('your', 'PRP$'), ('passion', 'NN'), ('for', 'IN'), ('the', 'DT'), ('outdoors', 'NNS'), ('and', 'CC'), ('introduce', 'VB'), ('a', 'DT'), ('friend', 'NN'), ('to', 'TO'), ('hike', 'VB'), ('and', 'CC'), ('camp', 'VB'), ('--', ':'), ('do', 'VB'), (\"n't\", 'RB'), ('forget', 'VB'), ('to', 'TO'), ('start', 'VB'), ('slow', 'JJ'), ('.', '.')]\n",
      "[('Online Shopping for Kitchen Small', 'NNP'), ('Appliances', 'NNPS'), ('from', 'IN'), ('a', 'DT'), ('great', 'JJ'), ('selection', 'NN'), ('of', 'IN'), ('Coffee Machines', 'NNP'), (',', ','), ('Blenders', 'NNP'), (',', ','), ('Juicers', 'NNP'), (',', ','), ('Ovens', 'NNP'), (',', ','), ('Specialty', 'NNP'), ('Appliances', 'NNPS'), (',', ','), ('&', 'CC'), ('more', 'RBR'), ('at', 'IN'), ('everyday', 'NN'), ('low', 'JJ'), ('prices', 'NNS')]\n",
      "[('Edward Snowden', 'NNP'), ('might', 'MD'), ('never', 'RB'), ('live', 'VB'), ('in', 'IN'), ('the', 'DT'), ('U.S. as', 'NNP'), ('a', 'DT'), ('free', 'JJ'), ('man', 'NN'), ('again', 'RB'), ('after', 'IN'), ('leaking', 'VBG'), ('secrets', 'NNS'), ('about', 'IN'), ('a', 'DT'), ('U.S.', 'NNP'), ('surveillance', 'NN'), ('program', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(webpages)):\n",
    "    print reduced_pos_tag(webpages.values()[i].meta_description)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Shipped it to nlp module__\n",
    "\n",
    "Testing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Georgia Institute of Technology College of Computing', 'Wikipedia']\n",
      "['Barcelona News', 'Discussion /r/Barca']\n",
      "['Louis van Gaal', 'Manchester United', 'Marcos Rojo']\n",
      "['Barca', 'Atleti']\n",
      "[]\n",
      "['Introduce Your Indoorsy Friend', 'Outdoors']\n",
      "['Cuisinart CPT-122 Compact 2-Slice Toaster', 'Kitchen', 'Dining']\n",
      "['NSA']\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(webpages)):\n",
    "    print [x for x,y in nlp.reduced_pos_tag(webpages.values()[i].title) if y == 'NNP']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
